# NLP_Application

```
!jq 'del(.metadata.widgets)' Source.ipynb > Target.ipynb ## use this one in colab to avoid Github Metadata error
```

# Finetune for custom application 

[Training Small Language Model](https://github.com/AIAnytime/Training-Small-Language-Model/blob/main/Training_a_Small_Language_Model.ipynb)

[Multi-label classification](https://www.kaggle.com/code/owaiskhan9654/multi-label-classification-of-pubmed-articles#-Training-the-model)

[Prompt Prediction - Miztral,Gemma,Llama](https://www.kaggle.com/code/aatiffraz/prompt-prediction-w-mixtral-mistral7b-gemma-llama)

[Code Interpreter Baseline](https://www.kaggle.com/code/huikang/code-interpreter-baseline)

[Google Translate for NLP Augmentation](https://www.kaggle.com/code/tuckerarrants/using-google-translate-for-nlp-augmentation)

## JPX Tokyo Stock Exchange Prediction

Task - Predict Ranks by date and Securities Code (stocks). Rank indicates the stocks from highest to lowest expected returns and is evaluated on the difference in returns between the top and bottom 200 stocks. 

[Stock Exchange Prediction](https://www.kaggle.com/code/chumajin/easy-to-understand-the-competition)


## LMSYS - Chatbot Arena Human Preference Predictions

Task - Predict which responses users will prefer in a head-to-head battle between chatbots powered by large language models (LLMs)

[Notebooks 1 : Multi-part notebook - train,quantize,infer](https://www.kaggle.com/code/cdeotte/16th-place-train-1-of-3)
[Notebook 2 : Combine 2 models](https://www.kaggle.com/code/jaejohn/lmsys-combine-gemma-2-9b-llama-3-8b/notebook)

## Eedi - Mining Misconceptions in Mathematics

Task - Develop an NLP model driven by ML to accurately predict the affinity between misconceptions and incorrect answers (distractors) in MCQ

[Working with vLLM](https://www.kaggle.com/code/aerdem4/eedi-qwen32b-vllm-with-logits-processor-zoo/notebook)

